---
title: ""
output: 
  #html_document: default
  pdf_document: default
bibliography: academic_paper.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Living HEOR: Automating HTA with R {.tabset}

*Robert A Smith^1^ & Paul P Schneider^1^*

<br>

\begingroup\small
*^1^ Lumanity, Sheffield*  
*^2^ Dark Peak Analytics, Sheffield*   
*^3^ School of Health and Related Research, University of Sheffield*
\endgroup

<br>

  __Keywords__:  `HEOR`, `HTA`, `APIs`, `R`, `Plumber`.  
  
  __Intended Journal__: Wellcome Open Research

****  

## Introduction

The process of updating economic models is time-consuming and expensive, and often involves the transfer of sensitive data between parties. This paper aims to demonstrate how updates to models in the Health Economics & Outcomes Research (HEOR) industry can be conducted in a way that allows clients, primarily those in the pharmaceutical industry, to retain full control of their data. We continue on to show how to automate reporting as new information becomes available, without the transfer of data between parties.

## Method {.tabset}

We developed an automated analysis and reporting pipeline for health economic modelling and made the source code openly available on a GitHub repository. It consists of three parts:
-	An economic model is constructed by the consultant using pseudo data (i.e. random data, which has the same format as the real data).
-	On the company side, an application programming interface (API), generated using the R package plumber, is hosted on a server. An automated workflow is created. This workflow sends the economic model to the company API. The model is then run within the company server. The results are sent back to the consultant, and a (PDF) report is automatically generated using RMarkdown.
- This API hosts all sensitive data, so that data does not have to be provided to the consultant.
- All of these processes can be controlled through an RShiny app, based on the tutorial application in our previous paper [@smith2020making].

![Schematic showing the interaction between the Company API and the Consultant Automated Workflow](../app_files/www/process_diagram.PNG)

All of the scripts discussed in this paper, as well as the code for the demonstration app can be found contained within an open access  [GitHub repository](https://github.com/RobertASmithBresMed/plumberHE).

### The API

An application programming interface is a set of rules, in the form of code, that allow different computers to interact with one another in real time. Whereas user-interfaces such as those generated by *shiny* allow humans to interact with data, APIs are designed to enable computers to interact with data.

When a 'client' application wants to access data, it initiates an API call (*request*) via a web-server, to retrieve the data. If this request is deemed valid, the API makes a call to an external program/server, the server sends a response to the API with the data, and the API transfers the data to the 'client' application. In a sense, the API is the broker (or middle-man) between two systems.

There are numerous benefits to APIs:
- in aiding speed of collaboration between institutions, ensuring inputs and outputs are standardised so that applications can 'talk' to one another.
- in security, eliminating the necessity to share data manually (e.g. via email). All interaction with data can be logged and access can be restricted by passwords and by limiting IP address access.
- eliminating computational burden on the client side (since all computation is done on the API owner side.)

There are lots of different implementations of APIs, but the main focus of this paper is on **Partner APIs**, which are created to allow data transfer between two different institutions. This requires a medium level of security, usually through the creation of login keys that are shared with partners.

In the examples below we use JSON to pass information to and from our API.


#### Plumber

Insert description of plumber and what it does....

```{r, file='../darthAPI/plumber.R',eval = F, echo = T, class.source = 'fold-hide'}
```

#### RStudio Connect

Once you have an account, it is simple to deploy the API direct to [RStudio connect](https://www.rstudio.com/products/connect/) from the Rstudio IDE. RStudio have a blog on how to publish an API created using Plumber to RStudio connect [here](https://www.rstudio.com/blog/rstudio-1-2-preview-plumber-integration/#:~:text=%20Resources%20%201%20Creating%20an%20API.%20On,APIs%20defined%20in%20your%20project%20and...%20More%20)


### The model code

This model code has been amended from the DARTH group's open source Cohort state-transition model (the Sick-Sicker Model) which can be found in this [GitHub repository](https://github.com/DARTH-git/Cohort-modeling-tutorial/) and is discussed in @alarid2020cohort.

```{r, file='../R/darth_funcs.R', eval = F, echo = F}
```

### Automating the model run

#### Interact with API from RScript
```{r, file='../scripts/run_darthAPI.R',eval = F, echo = T}
```

#### Use GitHub actions to automate the process
```{r, file='../.github/workflows/auto_model_run.yml',eval = F, echo = T}
```

## Discussion

The method is relatively complex, and requires a strong understanding of R, APIs, RMarkdown and GitHub Actions. However, the end result is a process, which allows the consultant to conduct health economic (or any other) analyses on company data, without having direct access â€“ the company does not need to share their sensitive data. The workflow can be scheduled to run at defined time points (e.g. monthly), or when triggered by an event (e.g. an update to the underlying data or model code). Results are generated automatically and wrapped into a full report. Documents no longer need to be revised manually.

## Conclusions

This example demonstrates that it is possible, within a HEOR setting, to separate the health economic model from the data, and automate the main steps of the analysis pipeline. We believe this is the first application of this procedure for a HEOR project.

\newpage
## References
